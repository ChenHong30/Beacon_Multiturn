#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Callable, Optional, Union
import torch
from torch import nn

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import (
    GenericForQuestionAnswering,
    GenericForSequenceClassification,
    GenericForTokenClassification,
    GradientCheckpointingLayer,
)

from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    QuestionAnsweringModelOutput, # Added based on Qwen2 diff
    SequenceClassifierOutputWithPast, # Added based on Qwen2 diff
    TokenClassifierOutput, # Added based on Qwen2 diff
)

from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple, logging
from transformers.utils.generic import check_model_inputs
from transformers.models.qwen3.configuration_qwen3 import Qwen3Config





logger = logging.get_logger(__name__)


@use_kernel_forward_from_hub("RMSNorm")
class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Qwen3RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config

        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        elif hasattr(config, "rope_parameters") and config.rope_parameters is not None:
            self.rope_type = config.rope_parameters.get("rope_type", "default")
        else:
            self.rope_type = "default"

        rope_init_fn: Callable = self.compute_default_rope_parameters
        if self.rope_type != "default":
            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]
        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)

        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = inv_freq

    @staticmethod
    def compute_default_rope_parameters(
        config: Optional[Qwen3Config] = None,
        device: Optional["torch.device"] = None,
        seq_len: Optional[int] = None,
    ) -> tuple["torch.Tensor", float]:
        """
        Computes the inverse frequencies according to the original RoPE implementation
        Args:
            config ([`~transformers.PreTrainedConfig`]):
                The model configuration.
            device (`torch.device`):
                The device to use for initialization of the inverse frequencies.
            seq_len (`int`, *optional*):
                The current sequence length. Unused for this type of RoPE.
        Returns:
            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the
            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).
        """
        if hasattr(config, "rope_parameters") and config.rope_parameters is not None:
            base = config.rope_parameters["rope_theta"]
        else:
            base = getattr(config, "rope_theta", 1000000.0)

        dim = getattr(config, "head_dim", None) or config.hidden_size // config.num_attention_heads

        attention_factor = 1.0  # Unused in this type of RoPE

        # Compute the inverse frequencies
        inv_freq = 1.0 / (
            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)
        )
        return inv_freq, attention_factor

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.layer_type = config.layer_types[layer_idx] if hasattr(config, "layer_types") else None
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        
        # Beacon tokençš„ç‹¬ç«‹QKVæŠ•å½±çŸ©é˜µ
        # æ³¨æ„ï¼šè¿™é‡Œå…ˆç”¨é›¶åˆå§‹åŒ–ï¼Œåœ¨from_pretrained()ä¸­ä¼šæ£€æŸ¥missing_keysåå†å†³å®šæ˜¯å¦ä»åŸå§‹æŠ•å½±å¤åˆ¶
        # è¿™æ ·å¯ä»¥é¿å…ä»checkpointæ¢å¤æ—¶è¦†ç›–å·²è®­ç»ƒçš„beaconæƒé‡
        self.beacon_q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.beacon_k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.beacon_v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        # Beacon Output Projection (following official activation_beacon implementation)
        self.beacon_o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        # Independent Norms for Beacon (Crucial for Qwen3 Stability)
        self.beacon_q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)
        self.beacon_k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)

        # å…ˆç”¨é›¶åˆå§‹åŒ–beaconå‚æ•°ï¼Œé¿å…éšæœºåˆå§‹åŒ–å¯¼è‡´çš„ä¸ç¨³å®š
        # å®é™…åˆå§‹åŒ–ä¼šåœ¨from_pretrained()ä¸­æ ¹æ®missing_keyså†³å®š
        with torch.no_grad():
            self.beacon_q_proj.weight.data.zero_()
            self.beacon_k_proj.weight.data.zero_()
            self.beacon_v_proj.weight.data.zero_()
            self.beacon_o_proj.weight.data.zero_()
            if self.beacon_q_proj.bias is not None:
                self.beacon_q_proj.bias.data.zero_()
            if self.beacon_k_proj.bias is not None:
                self.beacon_k_proj.bias.data.zero_()
            if self.beacon_v_proj.bias is not None:
                self.beacon_v_proj.bias.data.zero_()
            if self.beacon_o_proj.bias is not None:
                self.beacon_o_proj.bias.data.zero_()
        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œé˜²æ­¢post_init()å†æ¬¡åˆå§‹åŒ–
        self.beacon_q_proj._is_hf_initialized = True
        self.beacon_k_proj._is_hf_initialized = True
        self.beacon_v_proj._is_hf_initialized = True
        self.beacon_o_proj._is_hf_initialized = True
        self.beacon_q_norm._is_hf_initialized = True
        self.beacon_k_norm._is_hf_initialized = True

        self.sliding_window = config.sliding_window if self.layer_type == "sliding_attention" else None

    def _init_beacon_proj(self, missing_keys):
        """
        Initialize beacon projection weights from original projections.
        Only called when beacon weights are missing (not loaded from checkpoint).

        Args:
            missing_keys: List of missing keys from model loading
        """
        # åªæœ‰å½“beaconæƒé‡ç¼ºå¤±æ—¶æ‰ä»åŸå§‹æŠ•å½±å¤åˆ¶
        if any("beacon_q_proj" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_q_proj.weight.data.copy_(self.q_proj.weight.data)
                if self.beacon_q_proj.bias is not None and self.q_proj.bias is not None:
                    self.beacon_q_proj.bias.data.copy_(self.q_proj.bias.data)

        if any("beacon_k_proj" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_k_proj.weight.data.copy_(self.k_proj.weight.data)
                if self.beacon_k_proj.bias is not None and self.k_proj.bias is not None:
                    self.beacon_k_proj.bias.data.copy_(self.k_proj.bias.data)

        if any("beacon_v_proj" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_v_proj.weight.data.copy_(self.v_proj.weight.data)
                if self.beacon_v_proj.bias is not None and self.v_proj.bias is not None:
                    self.beacon_v_proj.bias.data.copy_(self.v_proj.bias.data)

        if any("beacon_q_norm" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_q_norm.weight.data.copy_(self.q_norm.weight.data)

        if any("beacon_k_norm" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_k_norm.weight.data.copy_(self.k_norm.weight.data)

        if any("beacon_o_proj" in key for key in missing_keys):
            with torch.no_grad():
                self.beacon_o_proj.weight.data.copy_(self.o_proj.weight.data)
                if self.beacon_o_proj.bias is not None and self.o_proj.bias is not None:
                    self.beacon_o_proj.bias.data.copy_(self.o_proj.bias.data)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        beacon_positions: Optional[torch.Tensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        # æ ¹æ®æ˜¯å¦æœ‰beaconä½ç½®ä¿¡æ¯æ¥å†³å®šä½¿ç”¨å“ªå¥—æŠ•å½±çŸ©é˜µ
        if beacon_positions is not None:
            # åˆ›å»ºbeacon mask
            beacon_mask = beacon_positions  # [batch_size, seq_len] - beacon_positionså·²ç»æ˜¯å¸ƒå°”å¼ é‡
            
            # Beacon Projections (Using INDEPENDENT Norms!)
            query_beacon = self.beacon_q_norm(self.beacon_q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            key_beacon = self.beacon_k_norm(self.beacon_k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            value_beacon = self.beacon_v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

            # Standard Projections
            query_normal = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            key_normal = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            value_normal = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            
            # æ ¹æ®beacon_maské€‰æ‹©ä½¿ç”¨å“ªå¥—æŠ•å½±ç»“æœ
            # beacon_mask: [batch_size, seq_len]
            # éœ€è¦åˆ†åˆ«ä¸ºqueryã€keyã€valueæ‰©å±•maskï¼Œå› ä¸ºGQAä¸­å®ƒä»¬çš„å¤´æ•°ä¸åŒ
            
            # ä¸ºqueryæ‰©å±•mask (num_attention_heads)
            beacon_mask_q = beacon_mask.unsqueeze(1).unsqueeze(-1).expand(-1, query_beacon.shape[1], -1, query_beacon.shape[3])
            query_states = torch.where(beacon_mask_q, query_beacon, query_normal)
            
            # ä¸ºkeyå’Œvalueæ‰©å±•mask (num_key_value_heads)
            beacon_mask_kv = beacon_mask.unsqueeze(1).unsqueeze(-1).expand(-1, key_beacon.shape[1], -1, key_beacon.shape[3])
            key_states = torch.where(beacon_mask_kv, key_beacon, key_normal)
            value_states = torch.where(beacon_mask_kv, value_beacon, value_normal)
        else:
            query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
            value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

            # Fix for DDP unused parameters issue:
            # Even if no beacons are present, we must ensure beacon parameters participate in the graph
            # to avoid "unused parameter" errors in DDP.
            if self.training and self.beacon_q_proj.weight.requires_grad:
                dummy_beacon = (
                    self.beacon_q_proj.weight.sum() + 
                    self.beacon_k_proj.weight.sum() + 
                    self.beacon_v_proj.weight.sum() + 
                    self.beacon_o_proj.weight.sum() + 
                    self.beacon_q_norm.weight.sum() + 
                    self.beacon_k_norm.weight.sum()
                ) * 0.0
                query_states = query_states + dummy_beacon

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()

        # æ ¹æ®æ˜¯å¦æœ‰beaconä½ç½®ä¿¡æ¯æ¥å†³å®šä½¿ç”¨å“ªå¥—è¾“å‡ºæŠ•å½±çŸ©é˜µ
        if beacon_positions is not None:
            # beacon tokens ä½¿ç”¨ beacon_o_projï¼Œæ™®é€š tokens ä½¿ç”¨ o_proj
            beacon_mask = beacon_positions  # [batch_size, seq_len]

            ordinal_output = self.o_proj(attn_output)
            beacon_output = self.beacon_o_proj(attn_output)

            # æ‰©å±• mask: [batch_size, seq_len] -> [batch_size, seq_len, hidden_size]
            beacon_mask_expanded = beacon_mask.unsqueeze(-1).expand_as(ordinal_output)
            attn_output = torch.where(beacon_mask_expanded, beacon_output, ordinal_output)
        else:
            attn_output = self.o_proj(attn_output)

        return attn_output, attn_weights


class Qwen3DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        beacon_positions: Optional[torch.Tensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            beacon_positions=beacon_positions,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@auto_docstring
class Qwen3PreTrainedModel(PreTrainedModel):
    config: Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3DecoderLayer,
        "attentions": Qwen3Attention,
    }


@auto_docstring
class Qwen3Model(Qwen3PreTrainedModel):
    def __init__(self, config: Qwen3Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types
        
        # å¤šè½®å¯¹è¯ç›¸å…³çš„ç‰¹æ®Štoken ID
        self.im_start_id = getattr(config, "im_start_token_id", 151644)
        self.im_end_id = getattr(config, "im_end_token_id", 151645)
        self.system_id = getattr(config, "system_token_id", 8948)
        self.user_id = getattr(config, "user_token_id", 872)
        self.assistant_id = getattr(config, "assistant_token_id", 77091)

        # Beacon token ä½¿ç”¨ç‹¬ç«‹çš„ embeddingï¼Œä¸æ‰©å±•è¯è¡¨
        # ä½¿ç”¨ vocab_size ä½œä¸ºæ ‡è¯†ç¬¦ï¼Œä½†å®é™… embedding æ¥è‡ªç‹¬ç«‹å‚æ•°
        self.beacon_token_id = self.vocab_size
        self.beacon_embedding = nn.Parameter(torch.empty(config.hidden_size))

        # === Beacons å¢å¼ºï¼šLearnable Position Embedding ===
        # æ¯ä¸ª beacon ä½ç½®æœ‰ç‹¬ç«‹çš„å¯å­¦ä¹  embeddingï¼Œå¸®åŠ©æ¨¡å‹åŒºåˆ†ä¸åŒ beacons çš„è§’è‰²
        # ä» config è¯»å– beacon æ•°é‡
        self.num_beacons_per_segment = getattr(config, 'num_beacons_per_segment', 16)
        # å¦‚æœconfigä¸­æ²¡æœ‰è®¾ç½®ï¼Œä¸”getattré»˜è®¤å€¼ä¹Ÿæœªç”Ÿæ•ˆ(ç†è®ºä¸Šä¸ä¼š)ï¼Œåˆ™æŠ¥é”™
        if self.num_beacons_per_segment is None:
            raise ValueError("num_beacons_per_segment must be specified in config")

        # ä» config è¯»å– sink token æ•°é‡ï¼ˆæ¯è½®æ¬¡ä¿ç•™çš„å¤´éƒ¨tokenæ•°ï¼‰
        self.num_sinks = getattr(config, 'num_sinks', 4)

        self.beacon_position_embedding = nn.Parameter(
            torch.zeros(self.num_beacons_per_segment, config.hidden_size)
        )

        # ç”¨äºè®°å½•å‹ç¼©åçš„ KV cache é•¿åº¦ï¼Œä¾›ç”Ÿæˆé˜¶æ®µä½¿ç”¨
        self._compressed_seq_length = None
        # ç”¨äºè®°å½•å‹ç¼©å‰çš„åŸå§‹åºåˆ—é•¿åº¦ï¼Œä¾›ç”Ÿæˆé˜¶æ®µè®¡ç®— position_ids
        self._original_seq_length = None

        # Initialize weights and apply final processing
        self.post_init()

    def extend_embeddings_for_beacon(self, tokenizer=None):
        """
        åˆå§‹åŒ– beacon_embedding å‚æ•°ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åè°ƒç”¨ã€‚
        ä½¿ç”¨è¯­ä¹‰ç›¸å…³è¯æ±‡ï¼ˆsummary/compress/overviewç­‰ï¼‰çš„embeddingå¹³å‡å€¼ä½œä¸ºåˆå§‹å€¼ã€‚

        Args:
            tokenizer: å¯é€‰çš„tokenizerï¼Œç”¨äºè·å–è¯­ä¹‰è¯æ±‡çš„token IDã€‚
                      å¦‚æœæœªæä¾›ï¼Œåˆ™å›é€€åˆ°ä½¿ç”¨ <|im_end|> çš„embeddingã€‚
        """
        with torch.no_grad():
            if tokenizer is not None:
                # ä½¿ç”¨è¯­ä¹‰ç›¸å…³è¯æ±‡çš„embeddingå¹³å‡å€¼
                semantic_words = ["summary", "compress", "overview", "condensed", "brief", "abstract"]
                embeddings_list = []

                for word in semantic_words:
                    # è·å–è¯æ±‡çš„token IDsï¼ˆå¯èƒ½è¢«åˆ†æˆå¤šä¸ªsubwordï¼‰
                    token_ids = tokenizer.encode(word, add_special_tokens=False)
                    if len(token_ids) > 0:
                        # å–ç¬¬ä¸€ä¸ªsubwordçš„embeddingï¼ˆé€šå¸¸æ˜¯ä¸»è¦è¯­ä¹‰ï¼‰
                        word_embedding = self.embed_tokens.weight[token_ids[0]]
                        embeddings_list.append(word_embedding)

                if len(embeddings_list) > 0:
                    # è®¡ç®—æ‰€æœ‰è¯­ä¹‰è¯æ±‡embeddingçš„å¹³å‡å€¼
                    avg_embedding = torch.stack(embeddings_list).mean(dim=0)
                    self.beacon_embedding.copy_(avg_embedding)
                    print(f"\033[93m[Beacon Init] Using average embedding of {len(embeddings_list)} semantic words\033[0m")
                else:
                    # å›é€€ï¼šä½¿ç”¨ <|im_end|> çš„embedding
                    im_end_embedding = self.embed_tokens.weight[self.im_end_id]
                    self.beacon_embedding.copy_(im_end_embedding)
                    print(f"\033[93m[Beacon Init] Fallback to <|im_end|> embedding\033[0m")
            else:
                # æ²¡æœ‰tokenizeræ—¶ï¼Œå›é€€åˆ°ä½¿ç”¨ <|im_end|> çš„embedding
                im_end_embedding = self.embed_tokens.weight[self.im_end_id]
                self.beacon_embedding.copy_(im_end_embedding)
                print(f"\033[93m[Beacon Init] Using <|im_end|> embedding (no tokenizer provided)\033[0m")

            # åˆå§‹åŒ– beacon position embeddingï¼ˆä½¿ç”¨å°çš„éšæœºå€¼ï¼‰
            # è¿™è®©æ¯ä¸ª beacon ä½ç½®æœ‰ç‹¬ç‰¹çš„åˆå§‹åŒ–ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ†å·¥
            nn.init.normal_(self.beacon_position_embedding, mean=0.0, std=0.02)

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    def parse_multiturn_dialogue(
        self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None
    ) -> tuple[list, torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """
        è§£æå¤šè½®å¯¹è¯åºåˆ—ï¼Œè¯†åˆ«æ¯ä¸ªQAè½®æ¬¡ï¼Œå¹¶åœ¨æ¯ä¸ªå†å²è½®æ¬¡æœ«å°¾æ·»åŠ beacon token

        Beaconå‹ç¼©é€»è¾‘ï¼š
        - åªå‹ç¼©å†å²è½®æ¬¡ï¼Œä¸å‹ç¼©å½“å‰è½®æ¬¡
        - ä¸€ä¸ªå®Œæ•´çš„è½®æ¬¡ = useræ¶ˆæ¯ + assistantå›å¤
        - Systemæ¶ˆæ¯ä¸å‹ç¼©
        - å½“å‰ç”¨æˆ·çš„é—®é¢˜ï¼ˆæœ€åä¸€ä¸ªuseræ¶ˆæ¯ï¼‰ä¸å‹ç¼©

        ä¾‹å¦‚ï¼šSystem + U1 + A1 + U2 + A2 + U3 (å½“å‰é—®é¢˜)
        ä¼šå˜æˆï¼šSystem + [B_U1] + [B_A1] + [B_U2] + [B_A2] + U3
        å…¶ä¸­ [B_U1] è¡¨ç¤º U1 çš„ beacon åºåˆ— (é•¿åº¦ä¸º num_beacons_per_segment)

        Returns:
            qa_segments: æ¯ä¸ªQAè½®æ¬¡çš„èµ·å§‹å’Œç»“æŸä½ç½®åˆ—è¡¨
            modified_input_ids: æ·»åŠ äº†beacon tokençš„input_ids
            beacon_positions: beacon tokençš„ä½ç½®mask
            modified_labels: è‹¥æä¾›labelsï¼Œåˆ™è¿”å›ä¸modified_input_idså¯¹é½çš„labelsï¼ˆbeaconä½ç½®å¡«å……ä¸º-100ï¼‰
        """
        batch_size, seq_len = input_ids.shape
        qa_segments = []
        modified_input_ids_list = []
        beacon_positions_list = []
        modified_labels_list = [] if labels is not None else None
        pad_token_id = self.config.pad_token_id
        if pad_token_id is None:
            pad_token_id = self.config.eos_token_id
        if pad_token_id is None:
            pad_token_id = 0

        for batch_idx in range(batch_size):
            ids = input_ids[batch_idx].tolist()
            label_row = labels[batch_idx].tolist() if labels is not None else None
            segments = []
            modified_ids = []
            beacon_pos = []
            modified_label_ids = [] if labels is not None else None

            # æŸ¥æ‰¾æ‰€æœ‰çš„<|im_start|>å’Œ<|im_end|>ä½ç½®
            start_positions = [i for i, token_id in enumerate(ids) if token_id == self.im_start_id]
            end_positions = [i for i, token_id in enumerate(ids) if token_id == self.im_end_id]

            # é…å¯¹startå’Œendä½ç½®ï¼Œè¯†åˆ«æ¯ä¸ªæ¶ˆæ¯æ®µè½
            pairs = list(zip(start_positions, end_positions))

            # åˆ†æå¯¹è¯ç»“æ„ï¼Œè¯†åˆ«è§’è‰²
            message_roles = []
            for start_pos, end_pos in pairs:
                # ä¸¥æ ¼æ£€æŸ¥ <|im_start|> ç´§æ¥ç€çš„ä¸‹ä¸€ä¸ªtoken
                # ä¹‹å‰çš„é€»è¾‘: if self.system_id in segment_tokens: ... ä¼šå› ä¸ºå†…å®¹ä¸­åŒ…å«system/user/assistant tokenè€Œè¯¯åˆ¤
                if start_pos + 1 < len(ids):
                    role_id = ids[start_pos + 1]
                    if role_id == self.system_id:
                        message_roles.append("system")
                    elif role_id == self.user_id:
                        message_roles.append("user")
                    elif role_id == self.assistant_id:
                        message_roles.append("assistant")
                    else:
                        message_roles.append("unknown")
                else:
                    message_roles.append("unknown")

            # # DEBUG: æ‰“å°è§’è‰²æ£€æµ‹ç»“æœ
            # print(f"\033[94m[DEBUG roles] message_roles: {message_roles}\033[0m")
            # print(f"\033[94m[DEBUG roles] Checking token IDs - system_id: {self.system_id}, user_id: {self.user_id}, assistant_id: {self.assistant_id}\033[0m")

            # æ‰¾åˆ°æœ€åä¸€ä¸ªuseræ¶ˆæ¯çš„ç´¢å¼•ï¼ˆè¿™æ˜¯å½“å‰è½®æ¬¡çš„é—®é¢˜ï¼Œä¸åº”è¯¥è¢«å‹ç¼©ï¼‰
            last_user_idx = -1
            for i in range(len(message_roles) - 1, -1, -1):
                if message_roles[i] == "user":
                    last_user_idx = i
                    break

            # è®¡ç®—æœ‰å¤šå°‘ä¸ªå®Œæ•´çš„å†å²è½®æ¬¡ï¼ˆuser+assistantå¯¹ï¼‰
            # å†å²è½®æ¬¡ = æœ€åä¸€ä¸ªuserä¹‹å‰çš„æ‰€æœ‰ésystemæ¶ˆæ¯
            history_messages = []
            for i in range(len(message_roles)):
                if i < last_user_idx and message_roles[i] != "system":
                    history_messages.append(i)

            # # DEBUG: æ‰“å°å†å²è½®æ¬¡åˆ†æ
            # print(f"\033[94m[DEBUG roles] last_user_idx: {last_user_idx}, history_messages: {history_messages}\033[0m")

            # å¦‚æœæ²¡æœ‰å†å²æ¶ˆæ¯éœ€è¦å‹ç¼©ï¼Œç›´æ¥è¿”å›åŸå§‹è¾“å…¥
            if len(history_messages) == 0:
                modified_input_ids_list.append(ids)
                beacon_positions_list.append([0] * len(ids))
                qa_segments.append([])
                if labels is not None and modified_labels_list is not None:
                    modified_labels_list.append(label_row)
                continue

            current_pos = 0
            for i, (start_pos, end_pos) in enumerate(pairs):
                role_type = message_roles[i]

                # è®°å½•å½“å‰æ®µè½åœ¨modified_idsä¸­çš„èµ·å§‹ä½ç½®ï¼ˆç”¨äºmaskï¼‰
                segment_start_in_modified = len(modified_ids)

                # æ·»åŠ å½“å‰æ®µè½çš„token
                segment_tokens = ids[current_pos:end_pos + 1]
                modified_ids.extend(segment_tokens)
                beacon_pos.extend([0] * len(segment_tokens))
                if labels is not None and modified_label_ids is not None:
                    segment_labels = label_row[current_pos:end_pos + 1]
                    modified_label_ids.extend(segment_labels)

                # åªåœ¨å†å²æ¶ˆæ¯ï¼ˆésystemï¼Œä¸”åœ¨æœ€åä¸€ä¸ªuserä¹‹å‰ï¼‰åæ·»åŠ beacon
                if i in history_messages:
                    # Insert beacon tokens (æ•°é‡ç”± config æŒ‡å®š)
                    for beacon_idx in range(self.num_beacons_per_segment):
                        modified_ids.append(self.beacon_token_id)
                        beacon_pos.append(1)  # æ ‡è®°è¿™æ˜¯beacon tokenä½ç½®
                        if labels is not None and modified_label_ids is not None:
                            # æœ€åä¸€ä¸ªbeaconé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆé€šå¸¸æ˜¯<|im_start|>ï¼‰
                            # å…¶ä»–beaconä½ç½®ä¸å‚ä¸loss
                            if beacon_idx == self.num_beacons_per_segment - 1:
                                # è·å–segmentåçš„ä¸‹ä¸€ä¸ªtokenä½œä¸ºé¢„æµ‹ç›®æ ‡
                                next_token_pos = end_pos + 1
                                if next_token_pos < len(ids):
                                    modified_label_ids.append(ids[next_token_pos])
                                else:
                                    modified_label_ids.append(-100)  # æ²¡æœ‰ä¸‹ä¸€ä¸ªtoken
                            else:
                                modified_label_ids.append(-100)  # éæœ€åbeaconä¸å‚ä¸loss

                    # è®°å½• (æ®µè½åœ¨modifiedä¸­çš„èµ·å§‹ä½ç½®, beaconåœ¨modifiedä¸­çš„ä½ç½®)
                    # æŒ‡å‘è¿™ä¸€æ‰¹beaconsçš„æœ€åä¸€ä¸ª
                    segments.append((segment_start_in_modified, len(modified_ids) - 1))

                current_pos = end_pos + 1

            # æ·»åŠ å‰©ä½™çš„tokenï¼ˆå¦‚æ¢è¡Œç¬¦ç­‰ï¼‰
            if current_pos < len(ids):
                remaining_tokens = ids[current_pos:]
                modified_ids.extend(remaining_tokens)
                beacon_pos.extend([0] * len(remaining_tokens))
                if labels is not None and modified_label_ids is not None:
                    remaining_labels = label_row[current_pos:]
                    modified_label_ids.extend(remaining_labels)

            modified_input_ids_list.append(modified_ids)
            beacon_positions_list.append(beacon_pos)
            qa_segments.append(segments)
            if labels is not None and modified_labels_list is not None:
                modified_labels_list.append(modified_label_ids)

        # å°†åˆ—è¡¨è½¬æ¢ä¸ºtensorï¼Œéœ€è¦paddingåˆ°ç›¸åŒé•¿åº¦
        max_len = max(len(ids) for ids in modified_input_ids_list)

        padded_input_ids = []
        padded_beacon_pos = []
        padded_labels = [] if modified_labels_list is not None else None

        for index, (ids, beacon_pos) in enumerate(zip(modified_input_ids_list, beacon_positions_list)):
            # padding
            pad_len = max_len - len(ids)
            padded_ids = ids + [pad_token_id] * pad_len
            padded_pos = beacon_pos + [0] * pad_len

            padded_input_ids.append(padded_ids)
            padded_beacon_pos.append(padded_pos)

            if padded_labels is not None and modified_labels_list is not None:
                label_ids = modified_labels_list[index]
                pad_labels = label_ids + [-100] * pad_len
                padded_labels.append(pad_labels)

        modified_input_ids = torch.tensor(padded_input_ids, dtype=input_ids.dtype, device=input_ids.device)
        beacon_positions = torch.tensor(padded_beacon_pos, dtype=torch.bool, device=input_ids.device)
        modified_labels_tensor = None
        if padded_labels is not None:
            modified_labels_tensor = torch.tensor(padded_labels, dtype=labels.dtype, device=labels.device)

        # # DEBUG: æ‰“å°è§£æç»“æœ
        # total_beacons = beacon_positions.sum().item()
        # if total_beacons > 0:
        #     print(f"\033[95m[DEBUG parse_multiturn] Total beacons: {total_beacons}\033[0m")
        #     print(f"\033[95m[DEBUG parse_multiturn] Original seq_len: {input_ids.shape[1]}, Modified seq_len: {modified_input_ids.shape[1]}\033[0m")
        #     # æ‰“å°è§’è‰²æ£€æµ‹ä¿¡æ¯ï¼ˆåªé’ˆå¯¹ç¬¬ä¸€ä¸ªbatchï¼‰
        #     if qa_segments and len(qa_segments) > 0 and len(qa_segments[0]) > 0:
        #         print(f"\033[95m[DEBUG parse_multiturn] qa_segments[0]: {qa_segments[0]}\033[0m")
        #     # æ‰“å°beaconä½ç½®
        #     beacon_indices = torch.nonzero(beacon_positions[0], as_tuple=False).squeeze(-1)
        #     if beacon_indices.numel() > 0:
        #         print(f"\033[95m[DEBUG parse_multiturn] Beacon positions in modified seq: {beacon_indices.tolist()}\033[0m")

        return qa_segments, modified_input_ids, beacon_positions, modified_labels_tensor

    def compress_kv_cache(
        self, past_key_values: Cache, beacon_positions: torch.Tensor, qa_segments: list = None
    ) -> Cache:
        """
        å‹ç¼©KV cacheï¼Œä¿ç•™ System + sink tokens + beacon tokens + å½“å‰è½®æ¬¡çš„æ‰€æœ‰tokenã€‚

        å‹ç¼©é€»è¾‘ï¼š
        - è¾“å…¥: System + Q1 + A1 + [Beacons] + Q2 + A2 + [Beacons] + Q3 (å½“å‰query)
        - è¾“å‡º: System + [Sinks_Q1A1] + [Beacons] + [Sinks_Q2A2] + [Beacons] + Q3
        - å†å²è½®æ¬¡çš„bodyéƒ¨åˆ†è¢«ä¸¢å¼ƒï¼Œä½†ä¿ç•™å…¶å¤´éƒ¨sink tokenså’Œbeaconè¡¨ç¤º
        - Systemå’Œå½“å‰è½®æ¬¡Q3å®Œæ•´ä¿ç•™
        - num_sinks: æ¯ä¸ªå†å²è½®æ¬¡ä¿ç•™çš„å¤´éƒ¨tokenæ•°é‡

        é‡è¦è®¾è®¡å†³ç­– (2024-12 ä¿®æ­£):
        - ä¸åš RoPE ä½ç½®ä¿®æ­£ï¼ä¿æŒåŸå§‹ä½ç½®ç¼–ç 
        - è®­ç»ƒæ—¶: tokens ä½¿ç”¨è‡ªç„¶é€’å¢çš„ä½ç½®
        - æ¨ç†æ—¶: ä¿æŒç›¸åŒçš„ä½ç½®ï¼Œç”Ÿæˆçš„æ–° token ä»åŸå§‹åºåˆ—é•¿åº¦ç»§ç»­
        - è¿™æ ·è®­ç»ƒå’Œæ¨ç†æ—¶çš„ç›¸å¯¹ä½ç½®è·ç¦»å®Œå…¨ä¸€è‡´

        æ”¯æŒBatch > 1ï¼Œä¼šè‡ªåŠ¨å¤„ç†ä¸åŒæ ·æœ¬é•¿åº¦ä¸ä¸€è‡´çš„æƒ…å†µï¼ˆè¿›è¡Œpaddingï¼‰ã€‚
        """
        if past_key_values is None or not torch.any(beacon_positions):
            return past_key_values

        batch_size = beacon_positions.shape[0]
        seq_len = beacon_positions.shape[1]

        # ä¿å­˜åŸå§‹åºåˆ—é•¿åº¦ï¼ˆå‹ç¼©å‰ï¼‰ï¼Œç”¨äºç”Ÿæˆé˜¶æ®µè®¡ç®— position_ids
        self._original_seq_length = seq_len

        # å…¼å®¹æ–°æ—§ç‰ˆæœ¬çš„ DynamicCache API
        if hasattr(past_key_values, 'key_cache'):
            key_cache_list = past_key_values.key_cache
            value_cache_list = past_key_values.value_cache
        else:
            num_layers = len(past_key_values)
            key_cache_list = [past_key_values[i][0] for i in range(num_layers)]
            value_cache_list = [past_key_values[i][1] for i in range(num_layers)]

        device = key_cache_list[0].device
        dtype = key_cache_list[0].dtype
        kv_seq_len = key_cache_list[0].shape[2]
        head_dim = key_cache_list[0].shape[3]

        # 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬éœ€è¦ä¿ç•™çš„ç´¢å¼•
        batch_keep_indices = []
        max_keep_len = 0

        for b in range(batch_size):
            # è·å–beaconç´¢å¼•
            beacon_indices = torch.nonzero(beacon_positions[b], as_tuple=False).squeeze(-1)

            if len(beacon_indices.shape) == 0:  # å•ä¸ªå…ƒç´ æ—¶squeezeä¼šå˜æˆæ ‡é‡
                beacon_indices = beacon_indices.unsqueeze(0)

            if len(beacon_indices) > 0:
                # SystemèŒƒå›´
                system_end = 0
                if qa_segments is not None and len(qa_segments) > b and len(qa_segments[b]) > 0:
                    first_segment_start = qa_segments[b][0][0]
                    system_end = first_segment_start

                if system_end > 0:
                    system_indices = torch.arange(0, system_end, device=device)
                else:
                    system_indices = torch.tensor([], dtype=torch.long, device=device)

                # æ”¶é›†æ¯ä¸ªå†å²æ®µè½çš„sink tokenså’Œbeacon tokens
                segment_indices_list = [system_indices]

                if qa_segments is not None and len(qa_segments) > b:
                    for seg_idx, (seg_start, seg_end) in enumerate(qa_segments[b]):
                        # seg_start: æ®µè½å¼€å§‹ä½ç½®
                        # seg_end: æœ€åä¸€ä¸ªbeaconçš„ä½ç½®
                        # ç¬¬ä¸€ä¸ªbeaconçš„ä½ç½®: seg_end - (num_beacons_per_segment - 1)
                        first_beacon_pos = seg_end - (self.num_beacons_per_segment - 1)

                        # Sink tokens: æ®µè½å¼€å¤´çš„num_sinksä¸ªtoken
                        sink_end = min(seg_start + self.num_sinks, first_beacon_pos)
                        if sink_end > seg_start:
                            sink_indices = torch.arange(seg_start, sink_end, device=device)
                            segment_indices_list.append(sink_indices)

                        # Beacon tokens
                        beacon_start = first_beacon_pos
                        beacon_end = seg_end + 1
                        beacon_seg_indices = torch.arange(beacon_start, beacon_end, device=device)
                        segment_indices_list.append(beacon_seg_indices)

                # æœ€åä¸€ä¸ªbeaconçš„ä½ç½®
                last_beacon_pos = beacon_indices[-1].item()
                # å½“å‰è½®æ¬¡çš„èŒƒå›´
                current_turn_start = last_beacon_pos + 1
                current_turn_end = min(seq_len, kv_seq_len)

                # å½“å‰è½®æ¬¡çš„ç´¢å¼•
                if current_turn_start < current_turn_end:
                    current_turn_indices = torch.arange(current_turn_start, current_turn_end, device=device)
                    segment_indices_list.append(current_turn_indices)

                # åˆå¹¶æ‰€æœ‰ç´¢å¼•
                keep_indices = torch.cat(segment_indices_list)
            else:
                # æ²¡æœ‰beaconï¼Œä¿ç•™æ‰€æœ‰ï¼ˆä¸å‹ç¼©ï¼‰
                keep_indices = torch.arange(min(seq_len, kv_seq_len), device=device)

            batch_keep_indices.append(keep_indices)
            max_keep_len = max(max_keep_len, len(keep_indices))

        if max_keep_len == 0:
            return past_key_values

        # 2. å‡†å¤‡æ–°çš„Cacheå®¹å™¨ï¼Œç›´æ¥æå–KVï¼Œä¸åšRoPEä¿®æ­£
        compressed_cache = DynamicCache()
        num_layers = len(key_cache_list)

        for layer_idx in range(num_layers):
            original_key = key_cache_list[layer_idx]  # [batch, num_heads, seq_len, head_dim]
            original_value = value_cache_list[layer_idx]

            num_heads = original_key.shape[1]

            new_key_list = []
            new_value_list = []

            for b in range(batch_size):
                keep_indices = batch_keep_indices[b]
                keep_count = len(keep_indices)

                if keep_count > 0:
                    # ç›´æ¥æå–éœ€è¦ä¿ç•™çš„KVï¼Œä¸åšä»»ä½•ä½ç½®ä¿®æ­£
                    k_chunk = original_key[b, :, keep_indices, :]  # [num_heads, keep_count, head_dim]
                    v_chunk = original_value[b, :, keep_indices, :]

                    # DEBUGï¼šéªŒè¯å‹ç¼©åçš„ç»“æ„ï¼ˆåªæ‰“å°ç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªbatchï¼‰
                    if layer_idx == 0 and b == 0:
                        _system_end = qa_segments[b][0][0] if (qa_segments and len(qa_segments) > b and len(qa_segments[b]) > 0) else 0
                        _num_beacons = int(beacon_positions[b].sum().item())
                        _num_segments = len(qa_segments[b]) if qa_segments and len(qa_segments) > b else 0
                        _num_sinks_total = _num_segments * self.num_sinks
                        print(f"\033[92m[Compress] System: {_system_end}, Sinks: {_num_sinks_total}, Beacons: {_num_beacons}, Total: {keep_count}\033[0m")
                        print(f"\033[92m[Compress] Original seq_len: {seq_len}, No RoPE correction (positions preserved)\033[0m")

                    # Paddingå¤„ç†
                    if keep_count < max_keep_len:
                        pad_len = max_keep_len - keep_count
                        pad_k = torch.zeros((num_heads, pad_len, head_dim), device=device, dtype=dtype)
                        pad_v = torch.zeros((num_heads, pad_len, head_dim), device=device, dtype=dtype)
                        k_chunk = torch.cat([k_chunk, pad_k], dim=1)
                        v_chunk = torch.cat([v_chunk, pad_v], dim=1)
                else:
                    k_chunk = torch.zeros((num_heads, max_keep_len, head_dim), device=device, dtype=dtype)
                    v_chunk = torch.zeros((num_heads, max_keep_len, head_dim), device=device, dtype=dtype)

                new_key_list.append(k_chunk)
                new_value_list.append(v_chunk)

            new_key_batch = torch.stack(new_key_list, dim=0)
            new_value_batch = torch.stack(new_value_list, dim=0)
            compressed_cache.update(new_key_batch, new_value_batch, layer_idx)

        # ä¿å­˜å‹ç¼©åçš„é•¿åº¦ï¼Œä¾›ç”Ÿæˆé˜¶æ®µä½¿ç”¨
        compressed_len = compressed_cache.get_seq_length()
        self._compressed_seq_length = compressed_len

        return compressed_cache

    @check_model_inputs()
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        enable_beacon_compression: Optional[bool] = True,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        # å¤šè½®å¯¹è¯è§£æå’Œbeacon tokenå¤„ç†
        beacon_positions = None
        qa_segments = None
        modified_labels = None

        # æ£€æŸ¥æ˜¯å¦æ˜¯prefillé˜¶æ®µï¼špast_key_valuesä¸ºNone æˆ–è€… é•¿åº¦ä¸º0
        is_prefill = (past_key_values is None or
                      (hasattr(past_key_values, 'get_seq_length') and past_key_values.get_seq_length() == 0))

        # åªåœ¨é¢„å¡«å……é˜¶æ®µè¿›è¡Œbeaconå‹ç¼©
        if (input_ids is not None and enable_beacon_compression and is_prefill):
            # è§£æå¤šè½®å¯¹è¯å¹¶æ·»åŠ beacon token
            qa_segments, modified_input_ids, beacon_positions, modified_labels = self.parse_multiturn_dialogue(
                input_ids, labels
            )

            # å¦‚æœæ£€æµ‹åˆ°å¤šè½®å¯¹è¯ï¼Œä½¿ç”¨ä¿®æ”¹åçš„input_ids
            if torch.any(beacon_positions):
                input_ids = modified_input_ids
                if modified_labels is not None:
                    labels = modified_labels

        if inputs_embeds is None:
            # å¤„ç† beacon token çš„ embedding
            # beacon_token_id è¶…å‡ºè¯è¡¨èŒƒå›´ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if beacon_positions is not None and torch.any(beacon_positions):
                # å°† beacon_token_id ä¸´æ—¶æ›¿æ¢ä¸º 0ï¼Œé¿å…ç´¢å¼•è¶Šç•Œ
                safe_input_ids = input_ids.clone()
                beacon_mask = (input_ids == self.beacon_token_id)
                safe_input_ids[beacon_mask] = 0
                inputs_embeds = self.embed_tokens(safe_input_ids)

                inputs_embeds = inputs_embeds.clone()

                # === Beacons å¢å¼ºï¼šä¸ºæ¯ä¸ª beacon æ·»åŠ ç‹¬ç‰¹çš„ position embedding ===
                # beacon_embedding æ˜¯å…±äº«çš„åŸºç¡€ embedding
                # beacon_position_embedding æ ¹æ® beacon åœ¨ segment å†…çš„ä½ç½®æ·»åŠ 
                batch_size, seq_len = input_ids.shape
                for b in range(batch_size):
                    beacon_indices = torch.nonzero(beacon_mask[b], as_tuple=False).squeeze(-1)
                    if beacon_indices.numel() > 0:
                        # æ¯ num_beacons_per_segment ä¸ª beacons ä¸ºä¸€ç»„ï¼ˆå¯¹åº”ä¸€ä¸ª segmentï¼‰
                        for i, idx in enumerate(beacon_indices.tolist()):
                            pos_in_segment = i % self.num_beacons_per_segment
                            # åŸºç¡€ embedding + ä½ç½® embedding
                            inputs_embeds[b, idx] = self.beacon_embedding + self.beacon_position_embedding[pos_in_segment]
            else:
                inputs_embeds = self.embed_tokens(input_ids)
                
                # Fix for DDP unused parameters issue (Beacon Embeddings):
                if self.training and self.beacon_embedding.requires_grad:
                     dummy_beacon_emb = (
                         self.beacon_embedding.sum() + 
                         self.beacon_position_embedding.sum()
                     ) * 0.0
                     inputs_embeds = inputs_embeds + dummy_beacon_emb

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # å¦‚æœæ·»åŠ äº†beacon tokenï¼Œéœ€è¦è°ƒæ•´position_idså’Œcache_positionä»¥åŒ¹é…æ–°çš„åºåˆ—é•¿åº¦
        if beacon_positions is not None and torch.any(beacon_positions):
            # ç¡®ä¿position_idså’Œcache_positionçš„é•¿åº¦ä¸inputs_embedsåŒ¹é…
            if position_ids.shape[1] != inputs_embeds.shape[1]:
                # é‡æ–°è®¡ç®—position_idså’Œcache_positionä»¥åŒ¹é…å½“å‰åºåˆ—é•¿åº¦
                past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
                cache_position = torch.arange(
                    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1],
                    device=inputs_embeds.device
                )
                position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        # === Beacon Mask Logic (é˜²æ­¢è®­ç»ƒæ—¶"ä½œå¼Š") ===
        # å¼ºåˆ¶åç»­tokenæ— æ³•çœ‹åˆ°å†å²æ®µè½çš„åŸå§‹æ–‡æœ¬ï¼Œåªèƒ½çœ‹åˆ°beacon
        if qa_segments is not None and "full_attention" in causal_mask_mapping:
            current_mask = causal_mask_mapping["full_attention"]
            seq_len = inputs_embeds.shape[1]
            dtype = inputs_embeds.dtype
            min_val = torch.finfo(dtype).min

            # å¦‚æœcurrent_maskä¸ºNone (è¯´æ˜ä½¿ç”¨äº†éšå¼maskï¼Œä¾‹å¦‚FlashAttn)ï¼Œæˆ‘ä»¬éœ€è¦æ˜¾å¼åˆ›å»ºä¸€ä¸ªmaskæ¥ä¿®æ”¹
            if current_mask is None:
                # åˆ›å»ºæ ‡å‡†Causal Mask: ä¸Šä¸‰è§’ä¸º-infï¼Œä¸‹ä¸‰è§’ä¸º0
                # [1, 1, seq_len, seq_len]
                current_mask = torch.zeros((1, 1, seq_len, seq_len), device=inputs_embeds.device, dtype=dtype)
                upper_mask = torch.triu(torch.full_like(current_mask, min_val), diagonal=1)
                current_mask = current_mask + upper_mask
                causal_mask_mapping["full_attention"] = current_mask

            for b, segments in enumerate(qa_segments):
                for (start_i, end_i) in segments:
                    # start_i: æ®µè½å¼€å§‹
                    # end_i: æœ€åä¸€ä¸ª beacon çš„ä½ç½®
                    # ç¬¬ä¸€ä¸ª beacon çš„ä½ç½®: end_i - (num_beacons_per_segment - 1)
                    # Body èŒƒå›´: [start_i, first_beacon) (ä¸åŒ…å« beacons)

                    first_beacon = end_i - (self.num_beacons_per_segment - 1)  # ç¬¬ä¸€ä¸ª beacon çš„ä½ç½®

                    # åªæœ‰å½“Bodyéç©ºæ—¶æ‰éœ€è¦é®è”½
                    if start_i < first_beacon:
                        # 1. åç»­ tokens çœ‹ä¸åˆ°å†å² bodyï¼ˆåªèƒ½çœ‹åˆ° beaconsï¼‰
                        if end_i + 1 < seq_len:
                            current_mask[b, 0, end_i + 1:, start_i : first_beacon] = min_val

                            # åŒæ­¥æ›´æ–°sliding window mask (å¦‚æœå­˜åœ¨)
                            if "sliding_attention" in causal_mask_mapping and causal_mask_mapping["sliding_attention"] is not None:
                                causal_mask_mapping["sliding_attention"][b, 0, end_i + 1:, start_i : first_beacon] = min_val

                        # 2. === å…³é”®ä¿®å¤ï¼šè®©åŒä¸€ segment å†…çš„ beacons èƒ½åŒå‘ attend ===
                        # åœ¨ causal mask ä¸­ï¼Œåé¢çš„ beacon å·²ç»èƒ½çœ‹åˆ°å‰é¢çš„ beacon
                        # æˆ‘ä»¬éœ€è¦è®©å‰é¢çš„ beacon ä¹Ÿèƒ½çœ‹åˆ°åé¢çš„ beaconï¼ˆæ‰“ç ´ causal çº¦æŸï¼‰
                        # è¿™æ ·æ‰€æœ‰ beacons éƒ½èƒ½è·å¾—å®Œæ•´ä¿¡æ¯ï¼Œåä½œå‹ç¼©
                        for i in range(self.num_beacons_per_segment):
                            beacon_i = first_beacon + i
                            if beacon_i >= seq_len:
                                break
                            for j in range(i + 1, self.num_beacons_per_segment):  # è®© beacon_i èƒ½çœ‹åˆ°åé¢çš„ beacon_j
                                beacon_j = first_beacon + j
                                if beacon_j >= seq_len:
                                    break
                                # å…è®¸ beacon_i attend åˆ° beacon_jï¼ˆç§»é™¤ causal maskï¼‰
                                current_mask[b, 0, beacon_i, beacon_j] = 0.0

                                if "sliding_attention" in causal_mask_mapping and causal_mask_mapping["sliding_attention"] is not None:
                                    causal_mask_mapping["sliding_attention"][b, 0, beacon_i, beacon_j] = 0.0

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            hidden_states = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_embeddings=position_embeddings,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                beacon_positions=beacon_positions,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)

        # ç»´æŠ¤KV cacheå‹ç¼©ï¼ˆåªä¿ç•™beacon tokençš„KVï¼‰
        if (use_cache and enable_beacon_compression and beacon_positions is not None and
            torch.any(beacon_positions) and past_key_values is not None and past_key_values.get_seq_length() > 0):
            past_key_values = self.compress_kv_cache(past_key_values, beacon_positions, qa_segments)

        outputs = BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
        )
        outputs.beacon_positions = beacon_positions
        if labels is not None:
            outputs.adjusted_labels = labels

        return outputs


@auto_docstring
class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = {"lm_head.weight": "model.embed_tokens.weight"}
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        print("Using self-defined file.")
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Beacon compression é»˜è®¤å¯ç”¨
        self.enable_beacon_compression = True

        # Initialize weights and apply final processing
        self.post_init()
        
    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        **kwargs,
    ):
        """
        å‡†å¤‡ç”Ÿæˆæ‰€éœ€çš„è¾“å…¥ï¼Œç¡®ä¿ enable_beacon_compression å‚æ•°æ­£ç¡®ä¼ é€’ã€‚

        å…³é”®è®¾è®¡ (2024-12 ä¿®æ­£):
        - å‹ç¼©åä¸ä¿®æ­£ RoPEï¼Œä¿æŒåŸå§‹ä½ç½®ç¼–ç 
        - æ–°ç”Ÿæˆçš„ token çš„ position_ids ä»åŸå§‹åºåˆ—é•¿åº¦ç»§ç»­
        - è¿™æ ·è®­ç»ƒå’Œæ¨ç†æ—¶çš„ç›¸å¯¹ä½ç½®è·ç¦»å®Œå…¨ä¸€è‡´
        """
        # å¦‚æœæœ‰ past_key_valuesï¼Œè¯´æ˜æ˜¯ decoding é˜¶æ®µï¼Œåªå–æœ€åä¸€ä¸ª token
        if past_key_values is not None:
            if inputs_embeds is not None:
                input_ids = input_ids[:, -cache_position.shape[0]:]
            elif input_ids.shape[1] != cache_position.shape[0]:
                input_ids = input_ids[:, cache_position]

        # === å…³é”®ï¼šä½¿ç”¨åŸå§‹åºåˆ—é•¿åº¦è®¡ç®— position_ids ===
        # è¿™æ ·æ–° token çš„ä½ç½®ä»åŸå§‹åºåˆ—é•¿åº¦ç»§ç»­ï¼Œä¿æŒä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç›¸å¯¹è·ç¦»
        original_seq_len = getattr(self.model, '_original_seq_length', None)
        compressed_len = getattr(self.model, '_compressed_seq_length', None)

        if past_key_values is not None and original_seq_len is not None:
            # ç”Ÿæˆé˜¶æ®µï¼šposition_ids åº”è¯¥åŸºäºåŸå§‹åºåˆ—é•¿åº¦ï¼ˆå‹ç¼©å‰ï¼‰
            # è®¡ç®—å·²ç”Ÿæˆçš„æ–° token æ•°é‡
            kv_len = past_key_values.get_seq_length() if hasattr(past_key_values, 'get_seq_length') else 0
            # å·²ç”Ÿæˆçš„ token æ•° = å½“å‰ KV é•¿åº¦ - å‹ç¼©åçš„åˆå§‹é•¿åº¦
            generated_tokens = kv_len - compressed_len if compressed_len is not None else 0

            batch_size = input_ids.shape[0]
            seq_len = input_ids.shape[1]

            # æ–° token çš„ä½ç½® = åŸå§‹åºåˆ—é•¿åº¦ + å·²ç”Ÿæˆçš„ token æ•°
            start_pos = original_seq_len + generated_tokens
            position_ids = torch.arange(
                start_pos, start_pos + seq_len,
                dtype=torch.long, device=input_ids.device
            ).unsqueeze(0).expand(batch_size, -1)

            # cache_position ä»ç„¶åŸºäºå®é™…çš„ KV cache ä½ç½®ï¼ˆç”¨äº cache ç´¢å¼•ï¼‰
            cache_position = torch.arange(
                kv_len, kv_len + seq_len,
                device=input_ids.device
            )
        elif attention_mask is not None and position_ids is None:
            # Prefill é˜¶æ®µæˆ–æœªå‹ç¼©æ—¶ï¼Œä½¿ç”¨æ ‡å‡†é€»è¾‘
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values is not None:
                position_ids = position_ids[:, -input_ids.shape[1]:]
                position_ids = position_ids.clone(memory_format=torch.contiguous_format)

        # å¤„ç† inputs_embedsï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä½¿ç”¨ï¼‰
        if inputs_embeds is not None and cache_position is not None and cache_position[0] == 0:
            model_inputs = {"inputs_embeds": inputs_embeds, "input_ids": None}
        else:
            model_inputs = {"input_ids": input_ids.clone(memory_format=torch.contiguous_format), "inputs_embeds": None}

        # ä¼ é€’ cache_position æ¥è¿›è¡Œæ­£ç¡®çš„ mask åˆ‡ç‰‡
        if isinstance(past_key_values, Cache) and attention_mask is not None and attention_mask.ndim == 2:
            if model_inputs["inputs_embeds"] is not None:
                batch_size, sequence_length, _ = model_inputs["inputs_embeds"].shape
                device = model_inputs["inputs_embeds"].device
            else:
                batch_size, sequence_length = model_inputs["input_ids"].shape
                device = model_inputs["input_ids"].device

            # å½“ KV cache è¢«å‹ç¼©åï¼Œéœ€è¦è°ƒæ•´ attention_mask çš„é•¿åº¦
            if compressed_len is not None:
                # åˆ›å»ºæ–°çš„ attention_maskï¼Œé•¿åº¦ä¸ºå‹ç¼©åçš„ KV cache é•¿åº¦ + å½“å‰ token
                kv_len = past_key_values.get_seq_length() if hasattr(past_key_values, 'get_seq_length') else 0
                new_mask_len = kv_len + sequence_length
                attention_mask = torch.ones((batch_size, new_mask_len), dtype=torch.long, device=device)

            if hasattr(self.model, '_prepare_4d_causal_attention_mask_with_cache_position'):
                attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(
                    attention_mask,
                    sequence_length=sequence_length,
                    target_length=past_key_values.get_max_cache_shape(),
                    dtype=self.lm_head.weight.dtype,
                    device=device,
                    cache_position=cache_position,
                    batch_size=batch_size,
                )

        model_inputs.update(
            {
                "position_ids": position_ids,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "use_cache": use_cache,
                "attention_mask": attention_mask,
                # å…³é”®ï¼šä½¿ç”¨æ¨¡å‹å±æ€§ä¼ é€’è‡ªå®šä¹‰å‚æ•°ï¼ˆé¿å… kwargs è¢« generate() è¿‡æ»¤ï¼‰
                "enable_beacon_compression": getattr(self, 'enable_beacon_compression', True),
            }
        )
        return model_inputs

    def generate(self, input_ids=None, **kwargs):
        """
        é‡å†™ generate æ–¹æ³•ï¼Œç¡®ä¿ beacon compression å‚æ•°æ­£ç¡®è®¾ç½®
        """
        # é‡ç½®çŠ¶æ€å˜é‡ï¼Œç¡®ä¿æ¯æ¬¡ generate è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„
        self.model._original_seq_length = None
        self.model._compressed_seq_length = None

        # ä» kwargs ä¸­æå– enable_beacon_compressionï¼Œå¦‚æœæœ‰çš„è¯ç”¨å®ƒæ›´æ–°æ¨¡å‹å±æ€§
        if 'enable_beacon_compression' in kwargs:
            self.enable_beacon_compression = kwargs.pop('enable_beacon_compression')

        # è°ƒç”¨çˆ¶ç±»çš„ generate æ–¹æ³•
        return super().generate(input_ids=input_ids, **kwargs)

    @classmethod
    def from_pretrained(cls, *args, tokenizer=None, **kwargs):
        """
        é‡å†™from_pretrainedæ–¹æ³•ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡ååˆå§‹åŒ–beaconç›¸å…³å‚æ•°ã€‚

        åˆå§‹åŒ–æ—¶æœºä¿®å¤ (2024-12):
        - æ£€æŸ¥ missing_keys æ¥å†³å®šæ˜¯å¦éœ€è¦åˆå§‹åŒ–beaconå‚æ•°
        - å¦‚æœä»åŒ…å«beaconæƒé‡çš„checkpointåŠ è½½ï¼Œåˆ™ä¸è¦†ç›–å·²è®­ç»ƒçš„æƒé‡
        - å¦‚æœæ˜¯é¦–æ¬¡ä»åŸºç¡€æ¨¡å‹åŠ è½½ï¼Œåˆ™ä»åŸå§‹æŠ•å½±çŸ©é˜µå¤åˆ¶æƒé‡è¿›è¡Œwarm start

        Args:
            tokenizer: å¯é€‰çš„tokenizerï¼Œç”¨äºåˆå§‹åŒ–beacon embeddingæ—¶è·å–è¯­ä¹‰è¯æ±‡çš„embeddingã€‚
        """
        # è¯·æ±‚è¿”å›loading_infoä»¥è·å–missing_keys
        kwargs.update(output_loading_info=True)

        # è°ƒç”¨çˆ¶ç±»çš„from_pretrainedæ–¹æ³•
        result = super().from_pretrained(*args, **kwargs)

        # è§£æè¿”å›å€¼
        if isinstance(result, tuple):
            model, loading_info = result
        else:
            model = result
            loading_info = {"missing_keys": []}

        missing_keys = loading_info.get("missing_keys", [])

        # åªæœ‰å½“beaconç›¸å…³æƒé‡ç¼ºå¤±æ—¶æ‰åˆå§‹åŒ–
        # è¿™ç¡®ä¿ä»å·²è®­ç»ƒçš„beacon checkpointæ¢å¤æ—¶ä¸ä¼šè¦†ç›–æƒé‡
        has_missing_beacon_keys = any("beacon" in key for key in missing_keys)

        if has_missing_beacon_keys:
            # åˆå§‹åŒ– beacon_embedding å’Œ beacon_position_embedding
            # ä¼ å…¥tokenizerä»¥ä½¿ç”¨è¯­ä¹‰è¯æ±‡çš„embeddingå¹³å‡å€¼
            model.model.extend_embeddings_for_beacon(tokenizer=tokenizer)

            # åˆå§‹åŒ–æ¯å±‚çš„beaconæŠ•å½±çŸ©é˜µ
            for layer in model.model.layers:
                if hasattr(layer.self_attn, '_init_beacon_proj'):
                    layer.self_attn._init_beacon_proj(missing_keys)

            print(f"\033[93m[from_pretrained] Initialized beacon parameters from original projections (warm start)\033[0m")
        else:
            print(f"\033[92m[from_pretrained] Loaded beacon parameters from checkpoint (no re-initialization)\033[0m")

        # ç¡®ä¿beaconæŠ•å½±çŸ©é˜µå’Œbeacon_embeddingçš„æ•°æ®ç±»å‹ä¸æ¨¡å‹ä¸€è‡´
        target_dtype = model.model.embed_tokens.weight.dtype
        target_device = model.model.embed_tokens.weight.device

        # è½¬æ¢ beacon_embedding çš„æ•°æ®ç±»å‹
        model.model.beacon_embedding.data = model.model.beacon_embedding.data.to(
            dtype=target_dtype, device=target_device
        )

        # è½¬æ¢ beacon_position_embedding çš„æ•°æ®ç±»å‹
        model.model.beacon_position_embedding.data = model.model.beacon_position_embedding.data.to(
            dtype=target_dtype, device=target_device
        )

        # è½¬æ¢ beacon æŠ•å½±çŸ©é˜µçš„æ•°æ®ç±»å‹
        for layer in model.model.layers:
            if hasattr(layer.self_attn, 'beacon_q_proj'):
                layer.self_attn.beacon_q_proj = layer.self_attn.beacon_q_proj.to(dtype=target_dtype, device=target_device)
                layer.self_attn.beacon_k_proj = layer.self_attn.beacon_k_proj.to(dtype=target_dtype, device=target_device)
                layer.self_attn.beacon_v_proj = layer.self_attn.beacon_v_proj.to(dtype=target_dtype, device=target_device)
            if hasattr(layer.self_attn, 'beacon_o_proj'):
                layer.self_attn.beacon_o_proj = layer.self_attn.beacon_o_proj.to(dtype=target_dtype, device=target_device)
            if hasattr(layer.self_attn, 'beacon_q_norm'):
                layer.self_attn.beacon_q_norm = layer.self_attn.beacon_q_norm.to(dtype=target_dtype, device=target_device)
                layer.self_attn.beacon_k_norm = layer.self_attn.beacon_k_norm.to(dtype=target_dtype, device=target_device)

        return model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        enable_beacon_compression: Optional[bool] = True,
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        enable_beacon_compression (`bool`, *optional*, defaults to `True`):
            Controls whether multi-turn prompts are augmented with beacon tokens during the prefill stage so that the
            beacon projections and KV compression logic are exercised.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        
        # åœ¨ç”Ÿæˆé˜¶æ®µä¿æŒbeaconå‹ç¼©å¯ç”¨ï¼Œä»¥ç»´æŒå‹ç¼©çš„KVç¼“å­˜
        # ä½†åœ¨ç”Ÿæˆé˜¶æ®µä¸ä¼šæ·»åŠ æ–°çš„beacon tokenï¼Œåªç»´æŠ¤ç°æœ‰çš„beaconçŠ¶æ€
        # Only disable beacon compression during generation if we don't want to maintain compressed cache
        # For memory efficiency, we should keep beacon compression on to maintain compressed KV cache
        # However, during generation (when past_key_values is not None), we should not add new beacon tokens
        # The compression logic will only maintain existing beacon positions
        if past_key_values is not None:
            # During generation, we maintain the compressed beacon cache but don't add new beacon tokens
            # The beacon_positions will remain as they were from prefill, ensuring we only keep beacon KV states
            pass  # Keep enable_beacon_compression as is to maintain compressed cache
            
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            enable_beacon_compression=enable_beacon_compression,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            if hasattr(outputs, "adjusted_labels"):
                labels = outputs.adjusted_labels
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class Qwen3ForSequenceClassification(GenericForSequenceClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForTokenClassification(GenericForTokenClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForQuestionAnswering(GenericForQuestionAnswering, Qwen3PreTrainedModel):
    base_model_prefix = "transformer"  # For BC, where `transformer` was used instead of `model`


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3PreTrainedModel",
    "Qwen3Model",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
]