#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen2/modular_qwen2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen2.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
from typing import Callable, Optional, Union

import torch
from torch import nn
import re

# ä¿®æ”¹ç›¸å¯¹å¯¼å…¥ä¸ºç»å¯¹å¯¼å…¥
from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import auto_docstring, can_return_tuple, logging
from transformers.models.qwen2.configuration_qwen2 import Qwen2Config


logger = logging.get_logger(__name__)


class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen2Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True
        
        # åŸå§‹çš„QKVæŠ•å½±çŸ©é˜µ
        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)
        
        # Beacon tokençš„ç‹¬ç«‹QKVæŠ•å½±çŸ©é˜µ
        self.beacon_q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)
        self.beacon_k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.beacon_v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        
        # ä½¿ç”¨åŸå§‹æŠ•å½±çŸ©é˜µçš„æƒé‡åˆå§‹åŒ–beaconæŠ•å½±çŸ©é˜µ (Warm Start)
        with torch.no_grad():
            self.beacon_q_proj.weight.copy_(self.q_proj.weight)
            self.beacon_k_proj.weight.copy_(self.k_proj.weight)
            self.beacon_v_proj.weight.copy_(self.v_proj.weight)
            if self.beacon_q_proj.bias is not None and self.q_proj.bias is not None:
                self.beacon_q_proj.bias.copy_(self.q_proj.bias)
            if self.beacon_k_proj.bias is not None and self.k_proj.bias is not None:
                self.beacon_k_proj.bias.copy_(self.k_proj.bias)
            if self.beacon_v_proj.bias is not None and self.v_proj.bias is not None:
                self.beacon_v_proj.bias.copy_(self.v_proj.bias)
        
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        beacon_positions: Optional[torch.Tensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        # æ ¹æ®æ˜¯å¦æœ‰beaconä½ç½®ä¿¡æ¯æ¥å†³å®šä½¿ç”¨å“ªå¥—æŠ•å½±çŸ©é˜µ
        if beacon_positions is not None:
            # åˆ›å»ºbeacon mask
            beacon_mask = beacon_positions  # [batch_size, seq_len] - beacon_positionså·²ç»æ˜¯å¸ƒå°”å¼ é‡
            
            # å¯¹äºbeacon tokenä½¿ç”¨beaconæŠ•å½±çŸ©é˜µï¼Œå¯¹äºæ™®é€štokenä½¿ç”¨åŸå§‹æŠ•å½±çŸ©é˜µ
            query_beacon = self.beacon_q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            key_beacon = self.beacon_k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            value_beacon = self.beacon_v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            
            query_normal = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            key_normal = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            value_normal = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            
            # æ ¹æ®beacon_maské€‰æ‹©ä½¿ç”¨å“ªå¥—æŠ•å½±ç»“æœ
            # beacon_mask: [batch_size, seq_len]
            # éœ€è¦åˆ†åˆ«ä¸ºqueryã€keyã€valueæ‰©å±•maskï¼Œå› ä¸ºGQAä¸­å®ƒä»¬çš„å¤´æ•°ä¸åŒ
            
            # ä¸ºqueryæ‰©å±•mask (num_attention_heads)
            beacon_mask_q = beacon_mask.unsqueeze(1).unsqueeze(-1).expand(-1, query_beacon.shape[1], -1, query_beacon.shape[3])
            query_states = torch.where(beacon_mask_q, query_beacon, query_normal)
            
            # ä¸ºkeyå’Œvalueæ‰©å±•mask (num_key_value_heads)
            beacon_mask_kv = beacon_mask.unsqueeze(1).unsqueeze(-1).expand(-1, key_beacon.shape[1], -1, key_beacon.shape[3])
            key_states = torch.where(beacon_mask_kv, key_beacon, key_normal)
            value_states = torch.where(beacon_mask_kv, value_beacon, value_normal)
        else:
            # æ²¡æœ‰beaconä½ç½®ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨åŸå§‹æŠ•å½±çŸ©é˜µ
            query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # main diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


@use_kernel_forward_from_hub("RMSNorm")
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen2DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        beacon_positions: Optional[torch.Tensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            beacon_positions=beacon_positions,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


@auto_docstring
class Qwen2PreTrainedModel(PreTrainedModel):
    config_class = Qwen2Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn_3 = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, Qwen2RMSNorm):
            module.weight.data.fill_(1.0)


class Qwen2RotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen2Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class Qwen2Model(Qwen2PreTrainedModel):
    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen2RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types
        
        # å¤šè½®å¯¹è¯ç›¸å…³çš„ç‰¹æ®Štoken ID
        self.im_start_id = 151644  # <|im_start|>
        self.im_end_id = 151645    # <|im_end|>
        self.system_id = 8948      # system
        self.user_id = 872         # user
        self.assistant_id = 77091  # assistant
        
        # æ·»åŠ beacon tokenåˆ°è¯æ±‡è¡¨ï¼ˆä½¿ç”¨ä¸€ä¸ªæœªä½¿ç”¨çš„token IDï¼‰
        self.beacon_token_id = self.vocab_size  # ä½¿ç”¨è¯æ±‡è¡¨å¤§å°ä½œä¸ºbeacon token ID

        # Initialize weights and apply final processing
        self.post_init()
    
    def extend_embeddings_for_beacon(self):
        """
        æ‰©å±•embeddingå±‚ä»¥åŒ…å«beacon tokenï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åè°ƒç”¨
        """
        original_vocab_size = self.embed_tokens.num_embeddings
        if original_vocab_size <= self.beacon_token_id:
            # æ‰©å±•embeddingå±‚
            new_embeddings = nn.Embedding(self.beacon_token_id + 1, self.config.hidden_size, self.padding_idx)
            # å¤åˆ¶åŸæœ‰çš„æƒé‡å¹¶ä¿æŒæ•°æ®ç±»å‹ä¸€è‡´
            with torch.no_grad():
                new_embeddings.weight[:original_vocab_size] = self.embed_tokens.weight
                # ä½¿ç”¨ <|im_end|> çš„embeddingåˆå§‹åŒ–beacon token
                # è¿™æ ·æ¨¡å‹ä¼šå°†å…¶è§†ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„ç»“æŸ/æ€»ç»“ä¿¡å·ï¼Œè€Œä¸æ˜¯éšæœºå™ªå£°
                im_end_embedding = self.embed_tokens.weight[self.im_end_id]
                new_embeddings.weight[self.beacon_token_id] = im_end_embedding
            
            # ç¡®ä¿æ–°embeddingå±‚çš„æ•°æ®ç±»å‹ä¸åŸæ¥ä¸€è‡´
            new_embeddings = new_embeddings.to(dtype=self.embed_tokens.weight.dtype, device=self.embed_tokens.weight.device)
            self.embed_tokens = new_embeddings
            # æ›´æ–°vocab_size
            self.vocab_size = self.beacon_token_id + 1
            self.config.vocab_size = self.vocab_size

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value
    
    def parse_multiturn_dialogue(
        self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None
    ) -> tuple[list, torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """
        è§£æå¤šè½®å¯¹è¯åºåˆ—ï¼Œè¯†åˆ«æ¯ä¸ªQAè½®æ¬¡ï¼Œå¹¶åœ¨æ¯ä¸ªè½®æ¬¡æœ«å°¾æ·»åŠ beacon token
        
        Returns:
            qa_segments: æ¯ä¸ªQAè½®æ¬¡çš„èµ·å§‹å’Œç»“æŸä½ç½®åˆ—è¡¨
            modified_input_ids: æ·»åŠ äº†beacon tokençš„input_ids
            beacon_positions: beacon tokençš„ä½ç½®mask
            modified_labels: è‹¥æä¾›labelsï¼Œåˆ™è¿”å›ä¸modified_input_idså¯¹é½çš„labelsï¼ˆbeaconä½ç½®å¡«å……ä¸º-100ï¼‰
        """
        batch_size, seq_len = input_ids.shape
        qa_segments = []
        modified_input_ids_list = []
        beacon_positions_list = []
        modified_labels_list = [] if labels is not None else None
        pad_token_id = self.config.pad_token_id
        if pad_token_id is None:
            pad_token_id = self.config.eos_token_id
        if pad_token_id is None:
            pad_token_id = 0
        
        for batch_idx in range(batch_size):
            ids = input_ids[batch_idx].tolist()
            label_row = labels[batch_idx].tolist() if labels is not None else None
            segments = []
            modified_ids = []
            beacon_pos = []
            modified_label_ids = [] if labels is not None else None
            
            # æŸ¥æ‰¾æ‰€æœ‰çš„<|im_start|>å’Œ<|im_end|>ä½ç½®
            start_positions = [i for i, token_id in enumerate(ids) if token_id == self.im_start_id]
            end_positions = [i for i, token_id in enumerate(ids) if token_id == self.im_end_id]
            
            if len(start_positions) <= 1:
                # åªæœ‰ä¸€ä¸ªæˆ–æ²¡æœ‰å¯¹è¯è½®æ¬¡ï¼Œä¸è¿›è¡Œbeaconå¤„ç†
                modified_input_ids_list.append(ids)
                beacon_positions_list.append([0] * len(ids))
                qa_segments.append([])
                if labels is not None and modified_labels_list is not None:
                    modified_labels_list.append(label_row)
                continue
            
            # é…å¯¹startå’Œendä½ç½®
            pairs = list(zip(start_positions, end_positions))
            
            current_pos = 0
            for i, (start_pos, end_pos) in enumerate(pairs):
                # æ·»åŠ å½“å‰æ®µè½çš„token
                segment_tokens = ids[current_pos:end_pos + 1]
                modified_ids.extend(segment_tokens)
                beacon_pos.extend([0] * len(segment_tokens))
                if labels is not None and modified_label_ids is not None:
                    segment_labels = label_row[current_pos:end_pos + 1]
                    modified_label_ids.extend(segment_labels)
                
                # è¯†åˆ«æ®µè½ç±»å‹
                segment_content = segment_tokens[start_pos-current_pos:end_pos-current_pos+1] if start_pos >= current_pos else segment_tokens
                role_type = "æœªçŸ¥"
                if self.system_id in segment_content:
                    role_type = "system"
                elif self.user_id in segment_content:
                    role_type = "user"
                elif self.assistant_id in segment_content:
                    role_type = "assistant"
                
                # åœ¨æ¯ä¸ªQAè½®æ¬¡ç»“æŸåæ·»åŠ beacon tokenï¼Œä½†æ’é™¤systemæ®µè½å’Œæœ€åä¸€ä¸ªæ®µè½
                # systemæ®µè½ä¸åº”è¯¥è¢«å‹ç¼©ï¼Œæœ€åä¸€ä¸ªæ®µè½æ˜¯å½“å‰è½®æ¬¡
                if i < len(pairs) - 1 and role_type != "system":
                    modified_ids.append(self.beacon_token_id)
                    beacon_pos.append(1)  # æ ‡è®°è¿™æ˜¯beacon tokenä½ç½®
                    segments.append((current_pos, len(modified_ids) - 1))  # è®°å½•è¿™ä¸ªQAæ®µè½çš„èŒƒå›´
                    if labels is not None and modified_label_ids is not None:
                        modified_label_ids.append(-100)  # beaconä½ç½®ä¸å‚ä¸loss
                
                current_pos = end_pos + 1
            
            # æ·»åŠ å‰©ä½™çš„token
            if current_pos < len(ids):
                remaining_tokens = ids[current_pos:]
                modified_ids.extend(remaining_tokens)
                beacon_pos.extend([0] * len(remaining_tokens))
                if labels is not None and modified_label_ids is not None:
                    remaining_labels = label_row[current_pos:]
                    modified_label_ids.extend(remaining_labels)
            
            modified_input_ids_list.append(modified_ids)
            beacon_positions_list.append(beacon_pos)
            qa_segments.append(segments)
            if labels is not None and modified_labels_list is not None:
                modified_labels_list.append(modified_label_ids)
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºtensorï¼Œéœ€è¦paddingåˆ°ç›¸åŒé•¿åº¦
        max_len = max(len(ids) for ids in modified_input_ids_list)
        
        padded_input_ids = []
        padded_beacon_pos = []
        padded_labels = [] if modified_labels_list is not None else None
        
        for index, (ids, beacon_pos) in enumerate(zip(modified_input_ids_list, beacon_positions_list)):
            # padding
            pad_len = max_len - len(ids)
            padded_ids = ids + [pad_token_id] * pad_len
            padded_pos = beacon_pos + [0] * pad_len
            
            padded_input_ids.append(padded_ids)
            padded_beacon_pos.append(padded_pos)
            
            if padded_labels is not None and modified_labels_list is not None:
                label_ids = modified_labels_list[index]
                pad_labels = label_ids + [-100] * pad_len
                padded_labels.append(pad_labels)
        
        modified_input_ids = torch.tensor(padded_input_ids, dtype=input_ids.dtype, device=input_ids.device)
        beacon_positions = torch.tensor(padded_beacon_pos, dtype=torch.bool, device=input_ids.device)
        modified_labels_tensor = None
        if padded_labels is not None:
            modified_labels_tensor = torch.tensor(padded_labels, dtype=labels.dtype, device=labels.device)
        
        return qa_segments, modified_input_ids, beacon_positions, modified_labels_tensor
    
    def compress_kv_cache(self, past_key_values: Cache, beacon_positions: torch.Tensor) -> Cache:
        """
        å‹ç¼©KV cacheï¼Œåªä¿ç•™beacon tokençš„KVï¼Œå¹¶ä¿®æ­£å…¶ä½ç½®ç¼–ç ä»¥ä¿æŒç›¸å¯¹ä½ç½®ä¸€è‡´æ€§ã€‚
        æ”¯æŒBatch > 1ï¼Œä¼šè‡ªåŠ¨å¤„ç†ä¸åŒæ ·æœ¬beaconæ•°é‡ä¸ä¸€è‡´çš„æƒ…å†µï¼ˆè¿›è¡Œpaddingï¼‰ã€‚
        """
        if past_key_values is None or not torch.any(beacon_positions):
            return past_key_values
        
        batch_size = beacon_positions.shape[0]
        device = past_key_values.key_cache[0].device
        dtype = past_key_values.key_cache[0].dtype
        
        # 1. é¢„å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„beaconç´¢å¼•å’Œç›¸å…³ä¿¡æ¯
        batch_beacon_indices = []
        max_beacons = 0
        
        for b in range(batch_size):
            # è·å–å½“å‰æ ·æœ¬æ‰€æœ‰beaconçš„ç´¢å¼•
            indices = torch.nonzero(beacon_positions[b], as_tuple=False).squeeze(-1)
            batch_beacon_indices.append(indices)
            max_beacons = max(max_beacons, len(indices))
            
        if max_beacons == 0:
            return past_key_values

        # 2. å‡†å¤‡æ–°çš„Cacheå®¹å™¨
        compressed_cache = DynamicCache()
        num_layers = len(past_key_values.key_cache)
        
        for layer_idx in range(num_layers):
            # è·å–å½“å‰å±‚çš„KV [batch, num_heads, seq_len, head_dim]
            original_key = past_key_values.key_cache[layer_idx]
            original_value = past_key_values.value_cache[layer_idx]
            
            num_heads = original_key.shape[1]
            head_dim = original_key.shape[3]
            
            # å‡†å¤‡æ”¶é›†å‹ç¼©åçš„KV
            # åˆå§‹åŒ–ä¸º0 (Padding)
            new_key_list = []
            new_value_list = []
            
            for b in range(batch_size):
                indices = batch_beacon_indices[b]
                current_count = len(indices)
                
                if current_count > 0:
                    # æå–å½“å‰æ ·æœ¬çš„Beacon KV [num_heads, num_beacons, head_dim]
                    # æ³¨æ„ï¼šoriginal_key[b] shape is [num_heads, seq_len, head_dim]
                    k_chunk = original_key[b, :, indices, :]
                    v_chunk = original_value[b, :, indices, :]
                    
                    # === RoPE ä¿®æ­£é€»è¾‘ ===
                    # ç›®æ ‡ä½ç½®: 0, 1, ..., N-1
                    target_pos = torch.arange(current_count, device=device)
                    # åŸå§‹ä½ç½®: indices
                    # Delta = ç›®æ ‡ - åŸå§‹
                    delta = target_pos - indices
                    
                    # è·å–Deltaå¯¹åº”çš„cos/sin
                    # position_ids éœ€è¦ shape [1, seq_len]
                    delta_input = delta.unsqueeze(0)
                    cos, sin = self.rotary_emb(v_chunk, position_ids=delta_input)
                    
                    # å› ä¸ºapply_rotary_pos_embæœŸæœ›è¾“å…¥åŒ…å«batchç»´åº¦ï¼Œæˆ‘ä»¬éœ€è¦unsqueeze/squeeze
                    # k_chunk: [num_heads, num_beacons, head_dim] -> [1, num_heads, num_beacons, head_dim]
                    k_chunk_unsqueezed = k_chunk.unsqueeze(0)
                    
                    # å¯¹Keyè¿›è¡Œæ—‹è½¬ä¿®æ­£
                    # æ³¨æ„ï¼šæˆ‘ä»¬åªå…³å¿ƒKeyçš„ä¿®æ­£ï¼ŒValueä¸éœ€è¦æ—‹è½¬
                    _, k_chunk_corrected = apply_rotary_pos_emb(
                        k_chunk_unsqueezed, k_chunk_unsqueezed, cos, sin
                    )
                    
                    # ç§»é™¤ä¸´æ—¶çš„batchç»´åº¦
                    k_chunk = k_chunk_corrected.squeeze(0)
                    
                    # Paddingå¤„ç† (å¦‚æœå½“å‰beaconæ•°å°‘äºæœ€å¤§beaconæ•°)
                    if current_count < max_beacons:
                        pad_len = max_beacons - current_count
                        pad_k = torch.zeros((num_heads, pad_len, head_dim), device=device, dtype=dtype)
                        pad_v = torch.zeros((num_heads, pad_len, head_dim), device=device, dtype=dtype)
                        k_chunk = torch.cat([k_chunk, pad_k], dim=1)
                        v_chunk = torch.cat([v_chunk, pad_v], dim=1)
                
                else:
                    # å¦‚æœè¯¥æ ·æœ¬æ²¡æœ‰beaconï¼Œå…¨å¡«0
                    k_chunk = torch.zeros((num_heads, max_beacons, head_dim), device=device, dtype=dtype)
                    v_chunk = torch.zeros((num_heads, max_beacons, head_dim), device=device, dtype=dtype)
                
                new_key_list.append(k_chunk)
                new_value_list.append(v_chunk)
            
            # å †å å›Batchç»´åº¦ [batch, num_heads, max_beacons, head_dim]
            new_key_batch = torch.stack(new_key_list, dim=0)
            new_value_batch = torch.stack(new_value_list, dim=0)
            
            compressed_cache.key_cache.append(new_key_batch)
            compressed_cache.value_cache.append(new_value_batch)
            
        return compressed_cache

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        enable_beacon_compression: Optional[bool] = True,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithPast:
        r"""
        Args:
            labels (`torch.LongTensor`, *optional*):
                Training labels aligned with `input_ids`. When beacon tokens are injected, the labels are automatically
                expanded with `-100` at beacon positions.
            enable_beacon_compression (`bool`, *optional*, defaults to `True`):
                Whether to parse multi-turn dialogues and insert beacon tokens during the prefill stage.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
        if not isinstance(past_key_values, (type(None), Cache)):
            raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

        # å¤šè½®å¯¹è¯è§£æå’Œbeacon tokenå¤„ç†
        beacon_positions = None
        qa_segments = None
        
        # åªåœ¨é¢„å¡«å……é˜¶æ®µï¼ˆpast_key_valuesä¸ºNoneï¼‰è¿›è¡Œbeaconå‹ç¼©
        if (input_ids is not None and enable_beacon_compression and 
            past_key_values is None):
            # è§£æå¤šè½®å¯¹è¯å¹¶æ·»åŠ beacon token
            qa_segments, modified_input_ids, beacon_positions, modified_labels = self.parse_multiturn_dialogue(
                input_ids, labels
            )
            
            # å¦‚æœæ£€æµ‹åˆ°å¤šè½®å¯¹è¯ï¼Œä½¿ç”¨ä¿®æ”¹åçš„input_ids
            if torch.any(beacon_positions):
                input_ids = modified_input_ids
                if modified_labels is not None:
                    labels = modified_labels
                # print(f"é¢„å¡«å……é˜¶æ®µï¼šæ£€æµ‹åˆ°å¤šè½®å¯¹è¯ï¼Œæ·»åŠ äº† {torch.sum(beacon_positions).item()} ä¸ªbeacon token")
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache()

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
            
        # å¦‚æœæ·»åŠ äº†beacon tokenï¼Œéœ€è¦è°ƒæ•´position_idså’Œcache_positionä»¥åŒ¹é…æ–°çš„åºåˆ—é•¿åº¦
        if beacon_positions is not None and torch.any(beacon_positions):
            # ç¡®ä¿position_idså’Œcache_positionçš„é•¿åº¦ä¸inputs_embedsåŒ¹é…
            if position_ids.shape[1] != inputs_embeds.shape[1]:
                # é‡æ–°è®¡ç®—position_idså’Œcache_positionä»¥åŒ¹é…å½“å‰åºåˆ—é•¿åº¦
                past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
                cache_position = torch.arange(
                    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], 
                    device=inputs_embeds.device
                )
                position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        # === Beacon Mask Logic (é˜²æ­¢è®­ç»ƒæ—¶"ä½œå¼Š") ===
        # å¼ºåˆ¶åç»­tokenæ— æ³•çœ‹åˆ°å†å²æ®µè½çš„åŸå§‹æ–‡æœ¬ï¼Œåªèƒ½çœ‹åˆ°beacon
        if qa_segments is not None and "full_attention" in causal_mask_mapping:
            current_mask = causal_mask_mapping["full_attention"]
            seq_len = inputs_embeds.shape[1]
            dtype = inputs_embeds.dtype
            min_val = torch.finfo(dtype).min
            
            # å¦‚æœcurrent_maskä¸ºNone (è¯´æ˜ä½¿ç”¨äº†éšå¼maskï¼Œä¾‹å¦‚FlashAttn)ï¼Œæˆ‘ä»¬éœ€è¦æ˜¾å¼åˆ›å»ºä¸€ä¸ªmaskæ¥ä¿®æ”¹
            if current_mask is None:
                # åˆ›å»ºæ ‡å‡†Causal Mask: ä¸Šä¸‰è§’ä¸º-infï¼Œä¸‹ä¸‰è§’ä¸º0
                # [1, 1, seq_len, seq_len]
                current_mask = torch.zeros((1, 1, seq_len, seq_len), device=inputs_embeds.device, dtype=dtype)
                upper_mask = torch.triu(torch.full_like(current_mask, min_val), diagonal=1)
                current_mask = current_mask + upper_mask
                causal_mask_mapping["full_attention"] = current_mask
                
            for b, segments in enumerate(qa_segments):
                for (start_i, end_i) in segments:
                    # start_i: æ®µè½å¼€å§‹
                    # end_i: beaconä½ç½® (è¯¥æ®µè½çš„æœ€åä¸€ä¸ªtoken)
                    # BodyèŒƒå›´: [start_i, end_i) (ä¸åŒ…å«beacon)
                    
                    # åªæœ‰å½“Bodyéç©ºæ—¶æ‰éœ€è¦é®è”½
                    if start_i < end_i:
                        # é®è”½è§„åˆ™ï¼šå½“å‰æ®µè½ä¹‹åçš„æ‰€æœ‰token (end_i + 1 :) 
                        # ä¸èƒ½çœ‹åˆ°å½“å‰æ®µè½çš„Body (start_i : end_i)
                        # è¿™æ ·å¼ºåˆ¶æ¨¡å‹å¿…é¡»é€šè¿‡beacon (end_i) æ¥è·å–è¯¥æ®µè½çš„ä¿¡æ¯
                        if end_i + 1 < seq_len:
                            current_mask[b, 0, end_i + 1:, start_i : end_i] = min_val
                            
                            # åŒæ­¥æ›´æ–°sliding window mask (å¦‚æœå­˜åœ¨)
                            if "sliding_attention" in causal_mask_mapping and causal_mask_mapping["sliding_attention"] is not None:
                                causal_mask_mapping["sliding_attention"][b, 0, end_i + 1:, start_i : end_i] = min_val

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                beacon_positions=beacon_positions,
                **flash_attn_kwargs,
            )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)
        
        # åœ¨é¢„å¡«å……é˜¶æ®µå®Œæˆåï¼Œå‹ç¼©KV cacheï¼ˆåªä¿ç•™beacon tokençš„KVï¼‰
        # ç»´æŠ¤KV cacheå‹ç¼©ï¼ˆåªä¿ç•™beacon tokençš„KVï¼‰
        # åœ¨é¢„å¡«å……å®Œæˆåæˆ–ç”Ÿæˆé˜¶æ®µï¼Œå½“beaconå‹ç¼©å¯ç”¨ä¸”å­˜åœ¨beaconä½ç½®æ—¶è¿›è¡Œå‹ç¼©
        # é‡è¦ï¼šbeacon_positionsåœ¨prefillé˜¶æ®µç”Ÿæˆåï¼Œåœ¨ç”Ÿæˆé˜¶æ®µä¼šæŒç»­ç”¨äºKVå‹ç¼©
        if (use_cache and enable_beacon_compression and beacon_positions is not None and 
            torch.any(beacon_positions) and past_key_values is not None and past_key_values.get_seq_length() > 0):
            past_key_values = self.compress_kv_cache(past_key_values, beacon_positions)
            # print(f"é¢„å¡«å……é˜¶æ®µå®Œæˆï¼šKV cacheå·²å‹ç¼©ï¼Œåªä¿ç•™ {torch.sum(beacon_positions).item()} ä¸ªbeacon tokençš„KV")

        outputs = BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )
        outputs.beacon_positions = beacon_positions
        if labels is not None:
            outputs.adjusted_labels = labels

        return outputs


class KwargsForCausalLM(FlashAttentionKwargs): ...


@auto_docstring
class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        print("Using self-defined file.")
        self.model = Qwen2Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model
    
    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        """
        é‡å†™from_pretrainedæ–¹æ³•ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åæ‰©å±•embeddingå±‚
        """
        # è°ƒç”¨çˆ¶ç±»çš„from_pretrainedæ–¹æ³•
        model = super().from_pretrained(*args, **kwargs)
        
        # æ‰©å±•embeddingå±‚ä»¥åŒ…å«beacon token
        model.model.extend_embeddings_for_beacon()
         
        # ç¡®ä¿beaconæŠ•å½±çŸ©é˜µçš„æ•°æ®ç±»å‹ä¸æ¨¡å‹ä¸€è‡´
        target_dtype = model.model.embed_tokens.weight.dtype
        target_device = model.model.embed_tokens.weight.device
         
        for layer in model.model.layers:
            if hasattr(layer.self_attn, 'beacon_q_proj'):
                layer.self_attn.beacon_q_proj = layer.self_attn.beacon_q_proj.to(dtype=target_dtype, device=target_device)
                layer.self_attn.beacon_k_proj = layer.self_attn.beacon_k_proj.to(dtype=target_dtype, device=target_device)
                layer.self_attn.beacon_v_proj = layer.self_attn.beacon_v_proj.to(dtype=target_dtype, device=target_device)
        
        # åŒæ—¶éœ€è¦æ‰©å±•lm_headä»¥åŒ¹é…æ–°çš„vocab_size
        if model.model.vocab_size > model.lm_head.out_features:
            old_lm_head = model.lm_head
            new_lm_head = nn.Linear(model.config.hidden_size, model.model.vocab_size, bias=False)
             
            # å¤åˆ¶åŸæœ‰æƒé‡å¹¶ä¿æŒæ•°æ®ç±»å‹ä¸€è‡´
            with torch.no_grad():
                new_lm_head.weight[:old_lm_head.out_features] = old_lm_head.weight
                # ä½¿ç”¨ <|im_end|> çš„æƒé‡åˆå§‹åŒ–æ–°tokençš„è¾“å‡ºæƒé‡
                im_end_output_weight = old_lm_head.weight[model.model.im_end_id]
                # æ³¨æ„ï¼šlm_headçš„weight shapeæ˜¯ [vocab_size, hidden_size]
                # æˆ‘ä»¬éœ€è¦å°†è¿™ä¸€è¡Œå¤åˆ¶åˆ°æ–°ä½ç½®
                new_lm_head.weight[old_lm_head.out_features:] = im_end_output_weight.unsqueeze(0)
             
            # ç¡®ä¿æ–°lm_headçš„æ•°æ®ç±»å‹ä¸åŸæ¥ä¸€è‡´
            new_lm_head = new_lm_head.to(dtype=old_lm_head.weight.dtype, device=old_lm_head.weight.device)
            model.lm_head = new_lm_head
            model.vocab_size = model.model.vocab_size
            model.config.vocab_size = model.model.vocab_size
        
        return model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        enable_beacon_compression: Optional[bool] = True,
        **kwargs: Unpack[KwargsForCausalLM],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        enable_beacon_compression (`bool`, *optional*, defaults to `True`):
            Controls whether multi-turn prompts are augmented with beacon tokens during the prefill stage so that the
            beacon projections and KV compression logic are exercised.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen2ForCausalLM

        >>> model = Qwen2ForCausalLM.from_pretrained("meta-qwen2/Qwen2-2-7b-hf")
        >>> tokenizer = AutoTokenizer.from_pretrained("meta-qwen2/Qwen2-2-7b-hf")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # åœ¨ç”Ÿæˆé˜¶æ®µä¿æŒbeaconå‹ç¼©å¯ç”¨ï¼Œä»¥ç»´æŒå‹ç¼©çš„KVç¼“å­˜
        # ä½†åœ¨ç”Ÿæˆé˜¶æ®µä¸ä¼šæ·»åŠ æ–°çš„beacon tokensï¼Œåªç»´æŠ¤ç°æœ‰çš„beaconçŠ¶æ€
        # Only disable beacon compression during generation if we don't want to maintain compressed cache
        # For memory efficiency, we should keep beacon compression on to maintain compressed KV cache
        # However, during generation (when past_key_values is not None), we should not add new beacon tokens
        # The compression logic will only maintain existing beacon positions
        if past_key_values is not None:
            # During generation, we maintain the compressed beacon cache but don't add new beacon tokens
            # The beacon_positions will remain as they were from prefill, ensuring we only keep beacon KV states
            pass  # Keep enable_beacon_compression as is to maintain compressed cache
        
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            enable_beacon_compression=enable_beacon_compression,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            if hasattr(outputs, "adjusted_labels"):
                labels = outputs.adjusted_labels
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@auto_docstring(
    custom_intro="""
    The Qwen2 Model transformer with a sequence classification head on top (linear layer).

    [`Qwen2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """
)
class Qwen2ForSequenceClassification(Qwen2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen2Model(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> SequenceClassifierOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        transformer_outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        hidden_states = transformer_outputs.last_hidden_state
        logits = self.score(hidden_states)

        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        if self.config.pad_token_id is None:
            last_non_pad_token = -1
        elif input_ids is not None:
            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
        else:
            last_non_pad_token = -1
            logger.warning_once(
                f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
                "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
            )

        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )


@auto_docstring
class Qwen2ForTokenClassification(Qwen2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen2Model(config)
        if getattr(config, "classifier_dropout", None) is not None:
            classifier_dropout = config.classifier_dropout
        elif getattr(config, "hidden_dropout", None) is not None:
            classifier_dropout = config.hidden_dropout
        else:
            classifier_dropout = 0.1
        self.dropout = nn.Dropout(classifier_dropout)
        self.score = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> TokenClassifierOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.score(sequence_output)

        loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.config)

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@auto_docstring
class Qwen2ForQuestionAnswering(Qwen2PreTrainedModel):
    base_model_prefix = "transformer"

    def __init__(self, config):
        super().__init__(config)
        self.transformer = Qwen2Model(config)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.transformer.embed_tokens

    def set_input_embeddings(self, value):
        self.transformer.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        start_positions: Optional[torch.LongTensor] = None,
        end_positions: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        **kwargs,
    ) -> QuestionAnsweringModelOutput:
        outputs: BaseModelOutputWithPast = self.transformer(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        sequence_output = outputs.last_hidden_state

        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        loss = None
        if start_positions is not None and end_positions is not None:
            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)

        return QuestionAnsweringModelOutput(
            loss=loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "Qwen2PreTrainedModel",
    "Qwen2Model",
    "Qwen2ForCausalLM",
    "Qwen2ForSequenceClassification",
    "Qwen2ForTokenClassification",
    "Qwen2ForQuestionAnswering",
]
